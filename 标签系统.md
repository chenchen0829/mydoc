## 应用场景

​	

```
标签平台作为数据平台建设的一部分，底层涉及到了数据仓库离线/实时特征产出。上层对接了业务系统，并且提供统一的sdk.
	1. 对接投放(页面按钮，弹窗，红包个性化投放)，推送(站内信/IM消息等)等下游系统，通过统一的sdk，提供用户标签的查询。
	2. 用户画像，分析用户的特征，标签。
	3. 用户个性化行为(留存/复购)分析。
	4. A/B测试，用户增长，优化产品的使用体验。
	5. 外投，ROI/效果分析。
	6. 个人化推荐/相关商品推荐
	
```



## 核心架构

```
离线特征平台，实时特征平台， 标签管理平台，标签调度平台，服务接口平台， 分析平台6个平台组成。

难点：
1. 离线特征N+1数据的kpi产出。必须要保证早上10点之前完成es索引的更新，并且按照优先级完成标签数据的生成。
	后续可以采用分业务划分离线集群，把核心业务，金融，风控等根据优先级做不同的宽表。
	
2. 实时数据写入kv，又要支持实时数据的统计。样本的支持。	

3. 标签数据回流到hive。 下游业务数据回流hive做分析模型。

```





## 离线特征平台

```

1. 离线特征一般都是从hive获取，并且根据id打成宽表。因为电商业务的复杂性，hive表也根据业务进行划分(基础属性类，统计类(流量，成交，社交行为，直播)， 机器学习挖掘偏好类特征)。一般属性类的都是全量表（成交之类的可以根据业务情况用增量表）。机器挖掘类特征因为开发周期比较长，特征之间差异很大，一般一个特征一张表结构。单个特征一般需要多个字段来表示(偏好类根据分值来判断)
2. 离线特征的表结构一般在几十张左右，每张表的数据量都是亿级别，通过表关联来生成最后的宽表，效率低也难以维护。我们最初的版本做的是通过sql来组装的方式。即席查询通过prosto来做。整个系统的产出和调度通过数据平台的调度系统来搞,这种方式的好处在于简单，坏处在于如果sql复杂，presto即席查询可能失败率较高，特征的分布(max,min,中位数等分析)，标签的特征分布，标签对比全部都要重新写对应的sql模型去做。增加了复杂性。
3. 我们整体的做法是通过spark读取hive的数据通过buckload到一张hive表，如果是每日调度，每天生成一张新的htable。支持并发导入，因为buckload的特性，都是文件操作。导入效率很高。在所有hive表全部导入完毕之后，htable已经生成。通过spark-dump批量写入到es，并且更新索引。我们每天的全量数据在7亿左右，每天dump的时间在1个小时内。


1. 我们这在生成hive表的时候必须做到去重，否则dump到hbase中会报错。
2. spark-buckload-hbase, 支持并发操作，但是每张hive表的数据要保证字段不一致。否则会报错。每天必须创建新的htable表，目前我们是100个固定region,不支持分割，所有rowkey根据hash+前缀索引写入到不同的region预分区中，通过spark读取快照表的数据实现的(听说现在有直接读取快照的文件数据，暂时没搞过)。
3. 针对一些复杂场景，需要有维度作为辅助查询(比如分平台，分类目场景查询，userid和指标一对多)，目前我们在使用hive表的时候，会生成中间hive表，把一对多的关系变成一对一(辅助维度，指标生成json数组的形式，在es中保存成nested的类型)
```



## 实时特征平台

```
实时计算平台相对简单，所有的实时指标全部来源于实时数仓。实时数据有对应口径的离线特征作为补偿，以及后续进行数据校对，数据质量的保证。

整体的架构是，分业务通过flink实时处理，并且数据写入到特定的kafka。并且统一的实时数据处理引擎(打通特征元数据系统，通过元数据来管理实时数据的写入)处理。

实时数据处理引擎的主要功能是数据写入，以及流控， 由于需要实时统计实时标签的数据量，实时标签人群样本,在保证实时特征写入kv的同时，又写入到tidb，来方便做数据量统计。后续也可以扩展到写入到其他数据源。

实时数据一个困难点在于，需要离线数据的补偿，以及实时离线数据的核对，重刷等。每一个实时特征都需要一个专门的离线数据的脚本对应，在数据出现波动，实时/离线数据校对不正常时候，可以随时回刷离线数据保证线上业务的稳定。
```



## 特征元数据

```
特征元数据的管理：
1. 特征类型，前端展示类型(int(范围), String(手工输入，枚举选择), Date(固定时间，动态时间(google-code-Aviator表达式))
2. 包括离线/实时特征，来源，去向，口径，实时特征离线补偿脚本管理。
3. 离线特征分布，最大值，最小值，中间值，排除极值等。
```



## 标签创建/调度

```
标签创建
	标签一般有日更新，临时标签等。方式有很多种，特征组装(依赖或且的关系)，sql，文件等不同的方式组成标签。
	
	难点在于，特征的或且关系组合，可以有多层，后端可以保存组合的表达式关系。展示的时候根据表达式去解析。离线特征的查询依赖于es，目前我们是用es的dsl来做的。即席查询基本是3s以内。
	
标签的应用场景一般有推送/投放等，输出的数据方式有多中hdfs, hive表，kafka，kv.
	hdfs : 主要存储userid, did的数据。
	hive表 : 一般是通过es查询的结果，通过kafka回流到hive表，通过spark调用分析模型做一些偏算法的分析。
	kafka : 部分数据是通过直接写入到kafka，然后回流到其他的业务系统引擎中(比如商品标签直接回流到搜索引擎中)。
	kv : 部分投放的数据场景，因为涉及到用户端，需要直接写入到kv, 用户端在通过接口查询的方式获取用户标签，qps可以达到5w+(单机2000~3000左右)。
	
标签调度
	标签调度的核心是查询es,hive中的数据，写入到对应的存储中。复杂的场景需要链式写入(比如投放数据先写入hdfs,再写入kv)。
	
  我们的调度是通过消息来做的，上/下线,投放，推送等事件写入到kafka。通过调度引擎消费kafka的数据完成写入。
  
  难点在于：消息幂等型的控制，数据增量写入/下线删除。
  
  消息幂等型：
  	消息根据消息体定向到不同的分区，每台机器都是单线程消费消息体，然后通过加锁，根据状态机来做幂等，多线程进行处理。单机同事处理的任务数，根据任务大小，可以达到20~30之间。
  数据增量写入：
  	对于日更新的标签系统，需要保留写入hdfs历史文件，如果有增量写入kv的场景，会获取最新的文件和上一天的文件进行对比，删除历史的，保留最新的(bitMap, hashSet)。目前支持千万级别数据的对比。
```



## 标签的检索

```
标签的检索一般都是通过kv来做的。存储方式决定了查询的方式。kv以hash的方式作为存储模型。
1. 所有的离线特征存储到单独的字段中。
2. 所有实时特征存储到不同的字段中。
3. 混合特征分开处理，离线部分存储到单独的字段。

检索的方式：
	迭代所有的实时标签，和混合标签，通过标签或且关系的表达式组装对应的值。然后计算表达式的值。(这个地方使用了一个取巧的方式，直接计算Aviator表达式的值，需要对表达式进行预计算和缓存。否者会有fgc频繁的问题出现)


```



## 分析模型

```
目前支持的分析模型，有留存/复购，推送质量， 标签对比，标签特征分布情况(通过es直接可查询)等

留存/复购/推送质量，需要标签数据回流到hive，把业务方的曝光/点击数据回流到hive，然后套用不同的分析模型形成结果数据做各类型的展示。

比如推送质量，可以统计到用户查看/点击推送消息的情况，给出精准化推送建议。
```

