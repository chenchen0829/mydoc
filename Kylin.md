## OLAP

```
OLAP 联机分析处理，主要支持复杂的分析操作，侧重决策的支持，对大数据量比较友好。普遍的场景就是星状模型的数据表查询，并且支持上钻，下卷的一些功能。

目前OLAP有两个方向，
		1：预计算的方式。直接把所有维度组合的查询都预计算好，直接拿结果数据。
		2：直接查询数据明细方式，通过一些优化手段(内存计算，增加索引，数据字典等)，直接操作的是原始的数据。

目前在用的OLAP的一些开源引擎，kylin, druid, presto,impala, es比较多。druid主要用于实时数据，kylin,presto，impala主要用于离线，presto, impala主要基于内存计算，es既可以做离线数据导入，又可以做实时，偏明细类查询(复杂聚合，分页有点困难)。

kylin2.3版本之后增加了一些准实时的场景，kylin3.0版本增加了实时的OLAP的查询场景。

总的来讲，每一种开源引擎都有自己的适用场景(presto主要用于即席查询，依赖内存的大小，查询1~20s之间，druid主要用于实时数据宽表的实时统计,明细查询,查询0~3s。 kylin主要用于超大数据量，维度自由匹配查询，查询0~3s)。每一个引擎有自己的优势和缺点。
```

```
我们的一些经验，使用不同的工具用在自己擅长的区域内,下游通过统一数据服务对外透出。不同引擎对外是黑盒。
1. presto 即席查询，测试数据,因为是内存计算，所以无法提供稳定和高并发的服务。
2. kylin 实时/离线 多维度(10+)组合统计类查询。可以直接在业务系统中使用，也可以在自定义查询和可视化报表中使用。 数据存储到hbase中，大多数直接命中rowkey。可以提供稳定的高并发服务。
3. druid 偏实时类数据存储，查询。既可以查明细，又可以做汇总。支持一些复杂类(case when, 去重，topN，groupby类)的数据查询。 不过druid有一些很明显的缺点，架构比较复杂，有自己的独特存储引擎，现在依然是apache beta版本,社区不太活跃。后续很有可能被同类开源引擎超越。
4. es 主要用在离线的宽表数据查询。支持简单的汇总和明细类查询。尤其对大宽表数据友好。特征宽表可以存储在es，做汇总统计和明细查询。有一些实时类的明细数据，可以存储到es。
5. impala+kudu 没用过。没有话语权。
6. tidb 一个介于oltp/olap的开源引擎。olap部分存储类似于hbase的region。oltp好像是未来tidb的重点，支持事务，不需要mysql的分表。一个表就能搞定。 在亿级别的数据做olap没有任何压力。目前我们用在一些边缘的数据查询类服务上。性能相对比较稳定。

```



## kylin使用的场景

```
1. 官方推荐维度10个左右，可以做到自由组合，20+维度就需要做一些剪枝操作。指标数量不限(数据可以存储到hbase多个列族中)， 支持一些count-distinct(精确和模糊), topn, sum, avg, case when等。(上卷，下钻的场景，明细也支持，但是有一些限制)。

2. 支持离线/准实时/实时不同场景的OLAP
	准实时定时拉起最近offset到当前最大的offset之间的数据到m/r，做预计算。
	实时通过实时消费kafka数据，预计算保存到本地形成segment，定时合并segment到hbase。查询时候合并当前本地磁盘的数据和hbase中的历史数据。
	
3. 	稳定的业务
	字段不能经常发生变化，这个是kylin的死穴。字段变化在kylin里面只能重新建立cube。我们的做法是常见新的hive表或者视图来重新创建cube,(表名前缀保持一致)，待系统数据回刷完毕之后，下掉老的cube

	对于hive表，hive视图，kakfa的数据一般是先通过etl清洗之后，再落kylin做预计算，对于聚合指标字段尽量保证不为空。
	
4. TB级别数据数据量，查询毫秒级返回
	预计算的核心就是业务查询结果存储到hbase。 绝大多数的场景直接能命中hbase-rowkey. 
其他根据startKey, endKey,进行范围scan,效率比较高。
	我们的业务在千亿级别的数据量依然能保证s级返回。可以支持应用的下游业务系统，可视化报表等。有些根据不同维度组合做自定义查询。
```



## 部署方案

```
1. 线上升级到了最新的3.0.1(3.0.0有bug会导致OOM)版本，支持实时stream+olap。
2. 线上业务我们部署了三台服务器，读写分离，query节点做nginx负载均衡，一台作为主build服务，两台作为query查询。如果kylin3.0版本需要实时的场景，主build服务需要扩展到三台做实时部分数据存储备份。服务器的标准是12core+32G+500G。
3. 我这边设置了jvm的堆内存大小为24G，年轻代4G。在配置jvm参数时候遇到一个坑，加载kylin系统的时候，默认会把hbase的参数也读取过来，我这边hbase的参数默认使用了-XX:+UseG1GC的方式，这个地方有个坑，G1在jdk1.9为默认的配置，jdk1.8使用的时候会根据内存需要动态的指定年轻代(e区，s区)。因为业务会缓存大量的查询结果对象。在ygc的时候本来可以回收很多对象的，但是e,s区因为动态调整到很小的内存，对象会被转移到老年代。一个FGC 默认20ms。回收的频率赶不上增加的频率导致OOM:heap.
```



## 预计算

> 打个比方，一张表T有7个维度A,B,C需要查询count(*). 数据量100亿+。维度的组合，[A], [B], [C], [A,B], [A,C], [B,C],[A,B,C]。 我们经常使用的是[A,B],[B,C],[A,B,C]组合。于是我们把这些使用的维度组合的count(*)结果预先计算处理出来保存到hbase中。

#### 预计算流程

1. 把hive表或者接收kafka数据，以及lookup表，进行打平，构建一个宽表。

   ```
   一般来说，尽量在hive端做打平宽表的操作，为了方便通过sql进行查询。
   有些情况下会用到视图，其实也是ok的，注意一下数据倾斜问题。
   ```

2. 构建临时表(distribute by rand()) 防止数据倾斜使用， 默认不构建，为了加速创建

   

3. 把需要写入字典的数据通过m/r计算好，写入hdfs.

   ```
   1. 需要写入字典的字段不仅包含维度字段，也包含聚合类型为hll的字段。
   2. hdfs存储的主要是数据字典的的对象。后续再反序列化出来。
   3. 每一次任务都会生成单独的数据字典。
   4. reducer的个数和维度的个数保持一致。对于超大基数的维度，可以通过设置单独起m/r任务单独跑。
   ```

   

4. 把上一步的生成的数据字典，和老的数据字典进行合并，写入hbase,并且同步到缓存。

   ```
   一般来说维度类数据字典第一次生成完成之后，后续更新频率很低，其他的一些增量的数据需要与之前老的数据字典进行合并。
   ```

   

5. 创建HbaseTable

   ```
   1. 一般来说我们创建htable的时候，只需要指定分区策略，压缩策略，读缓存策略，预分区等。因为有增量数据的存在，必不可免的是要进行region的分裂和合并。但是kylin这个就有点特殊了，kylin的默认预分区的策略是DisabledRegionSplitPolicy(不分裂)，并且通过第三步骤m/r读取数据的时候，实现计算好表的总数据量，每一个cuboid的数据量大小。
   
   2. 由3步计算出所有cuboid的数据量字节数大小。计算hbase数据量的总存储大小，并且根据每一个region的大小regionCut(5~10G)划分region数(regionNum = totalSizeMB / regionCut)。
   
   3. 为了查询region做到均衡，每一个cuboid的数据都被一个特殊的"魔数",计算存储分片区域，一个cuboid都有几个分片组成。然后写入到rowkey的前缀，最后根据预分区的策略写入到不同的region。
   ```

   

6. 从N维到0维构建数据。

   ```
   比如A, B，C个维度，我们首先计算A,B,C组合的聚合结果，然后基于A,B,C 去掉C, 再次进行计算，就可以得到A,B的聚合结果。(每一次聚合都是基于上一次的聚合结果得到的)。
   	
   一个N维的Cube至少需要N次M/R任务才能聚合完成。
   ```

   

7. 将构建的数据转换成Hfile, 然后buckload到Hbase

   ```
   buckload已经是大数据的标配，尤其在批量初始化htable的表结构，做宽表的场景都用到很多。
   ```

   

8. 更新cube,清理垃圾数据。

   ```
   主要是第三步的一些hdfs数据字典的数据，hfile的数据。
   新的版本貌似直接可以清理，不过我们还是做通过crontab调用命令去清理一些job的垃圾数据，默认job的垃圾数据会一直存储，我这边一般设置7天。
   ```




#### 预计算剪枝

```
剪枝的目的就是为了在预计算的时候缩小维度组合，减少数据存储的压力。(比如10个维度，自由组合，共有2^10个组合方式。但是我们平时用的可能才几十中而已)。主要的剪枝方式

1. Includes (包含的维度)
		主要说一下includes中未在其他聚合类型(Mandatory Dimensions， Hierarchy Dimensions， Joint Dimensions)中使用的维度， 组成Cuboid的个数为2 ^N个
2. Mandatory Dimensions (强制维度)
	 - 这种维度嘴和是超规模优化剪枝最有效的手段。我在做20+维度的时候，有时候全部设置成强制维度，大概有上百组。
   - 生成的Cuboid组合数 1个。
   - 参考配置：kylin.cube.aggrgroup.is-mandatory-only-valid = true
3. Hierarchy Dimensions (分层维度)
		- 一般用在类似省/市/县联动维度场景，只能从上往下依次查询，不能从下往上。
    - 不建议配置多个组，多组之间会自由组合，如果有多个组，建议配置成多个Aggregation.
    - 生成cuboid N+1个。
4. Joint Dimensions (联合维度)
		联合维度，就是联合在一块使用的维度，每一组联合维度只会生成一个Cuboid, 如果有多组，多组之间会组合。
		用在一些维度不是必选，但是会放到一起使用的场景，另外一种是维度基数比较低，可以放到一起作为一个联合维度(空间换时间的概念)
		cuboid个数为2 * N 个。
		
整体算下来，,我们可以估计出Cuboid的总个数(1 * 2^M + N * 2 + (P + 1) 
	M为Incudes未被其他聚合组使用的维度个数
	N为联合维度的分组数，
	P为分层维度的分组数		
```

#### rowkey的组成

```
为什么要说一下rowkey的组成，因为kylin的最终数据源是hbase。日常我们使用hbase最大的问题就是要考虑存储,预分区，rowkey的设计。rowkey设计的好坏决定查询的速度。

下面的例子就是10个维度，按照rowkey的排序，组成一个二进制的数据(存储到hbase中的是二进制转long)，第一个维度存储在数据中第10个为主.  rowkey的结构 sharedKey(均衡每个region的数据) + Cuboid(rowkeybit数组转换的long) + 具体的维度值byte， 所有的字段都是先编码，再计算成long。
```

![image-20200515153016858](https://github.com/chenchen0829/mydoc/image/image-20200515153016858.png)



## 数据字典

> 无论是druid，还是kylin，为了降低存储，和查询效率，都用到了数据字典。数据字典在加速查询，降低存储有很大的用处。数据字典在大数据存储方面是一个神奇的存在。
>
> hdfs的不同存储格式都在考虑压缩，行列混合。进而减少磁盘占比。数据字典使用了另外一种方式，保存一份字段的映射到数据字典中。最终落到磁盘的全部都是映射数据。查询/保存数据的时候涉及到查询和同步数据字典。
>
> 所以需要考虑的点：1：数据字典写入速度，2：数据字典加载数据。从数据字典查询数据的速度决定着整个平台的查询性能。

#### 能够创建字典的字段

```
1. rowkey中字典类型为dict
2. measures 中count(distinct), 精确去重部分。 topN, raw类型。
3. global, segment Dictionary, 包含count(distinct) 精确去重。
```

#### Trid树

```
数字字典的原理就是用trid树保存整个维度，指标的值。并且用对应的int数据做mapping.
trid树出现的原因就是为了解决存储的问题，以及快速查询的问题，我们不可能把很多高基维度的数据全部存储到hbase作为rowkey来使用，这大大增加了rowkey的长度，kylin是做了一个映射，把维度值对应的映射byte数组转换成int存储到rowkey中。大大节约了空间，又提高了查询的效率。

1. trid树根节点不包含字符，其他节点每个只包含一个字符，且每个节点的子节点各不相同。
2. 从根节点到某一个节点，按照前序遍历，路径经过的字符连接起来，就是对应的字符串。
3. Trie树是一次性的构建全部的数据，维度映射的id对应于树节点的值。不能追加，追加之后由于树节点会转换，id变化。这个是Trie树的缺陷

TrieDictionary
	存储字符串，非排序类的维度和指标。通过int映射具体值的byte[]。 一般来说查询的性能比hashMap,数组慢10倍左右，但是可以大大减少存储空间。为了解决查询内置了本地缓存。为了防止加载过多的数据字典，会按需加载，并且本地缓存设置为软引用，在内部不足时候会被垃圾回收器回收。
	
DateStrDictionary
	存储日期类的数据。通过int映射具体的日期类型。
NumberDictionary
	存储数字类型的数据
TrieDictionaryForest
	trid树森林，当一个超高基数的维度存储到trie树中，内存占用量是相当可观的，TrieDictionaryForest利用数据的有序性(byteCompare),将一个大的Trid树切分为多颗小的Trie树，并且各棵树之间是保持相对有序的。在字典构建和字典加载时候都能按需压缩，反序列化。
AppendTrieDictionary
	相对于TrieDictionary的缺点, AppendTrieDictionary是可以追加的，原理就是把节点的值保存成对象的指针，即使树节点发生变化，节点值映射的id不会发生变化，
	其他的数据字典都是segment级别的。如果我们的表是分区表，按照日期进行分期，数据每天会产生一个hbase表(segment)。字典是对应于每一个分区。AppendTrieDictionary是可以追加的全局trie树(所有分区)。一般用于count(distinct)场景下。

```



#### Trid树序列化

```
trid树按照广度优先序列化，每个node都存储key，value，offset， 当需要从字典中检索某个值的时候，可直接序列化节点，并且根据offset进行读取。不需要反序列化整颗树。

```

![image-20200515171747603](https://github.com/chenchen0829/mydoc/image/image-20200515171747603.png)



## 去重

```
去重是整个大数据场景下必须面对的问题，在电商场景下订单级别的都是需要精确去重。流量场景下可以支持精确度高的模糊去重。去重的方式很多。
1. hashMap去重(支持少量的任意类型)
2. bitmap 原生支持int，short类型的数据去重。也可以支持string类型，不过需要通过数据字典先转换再存储。
3. hll算法模糊去重，通过调整hll的参数调整去重的精度。
精确去重的两个点：
1. 支持任意维度组合的上卷聚合。(所以对于任意一种组合，hbase中都需要存储明细数据的bitmap)，对于不需要上卷的一些场景，可以直接通过hbase的协处理器直接计算出结果，不需要返回server端。
2. String类型的通过数据字典的映射，可以做到int类型的bitmap。
```

#### RoaringBitMap

```
在实际场景下，bitmap存储占用的空间还是很大，1亿的歌32位的int， 大概需要1 0000 0000 / 8 / 1024 /1024 = 12M
但是绝大多数场景下，我们使用的都是很稀疏的数据。比如short或者数组就已经够用了，基于这种场景RoaringBitMap内置了三种container, 
	ArrayContainer(存储数据量<4096， 超过会转化成BitMapContaioner), 
	BitMapContaioner(存放稠密的数据，固定8kb.short类型), 
	RunContaioner(主要针对连续的数据。[11, 12, 13, 14, 15, 21, 22]会记录成11,4,21,2))。 
	
RoaringBitmap解决了实际场景下hbase存储浪费的问题，又通过进一步的压缩数据。数据存储可以进一步降低。
```

![image-20200427165559803](https://github.com/chenchen0829/mydoc/blob/master/image/image-20200427165559803.png)

#### HLL

```
1. 在模糊count(distinct)的场景下，HHL算法可以做到很高的精度，并且占用空间很小(hll(10)占用1kb)
2. 原理简单来讲就是把所有数据hash之后分到N个桶中，然后拿到某一个数据的hash，然后找出该hash值第一个1出现最晚的位置。为了减少误差设置hash值二进制N位，计算对应的桶。桶的个数据越多，占用空间就越大，精度就越高。
3. 具体算法比较复杂，在这就不在说了，想要了解的通过这个链接访问即可 https://m.sohu.com/a/312123816_659643
```

